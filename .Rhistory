tsplot(x, ylab = "Residual",  gg = T)
acf2(x, main = "Residual", gg = T)
#ACF shows seaonsal persistence so, we try differencing
tsplot(diff(x, 7), ylab = "Seasonal Differenced Residual", gg = T)
acf2(diff(x, 7), main = "Seasonal Differenced Residual", gg = T)
#ACF cuts off, PACF tails off, for seasonal
#So MA(1) models are chosen for seasonal.
#PACF cuts off, ACF tails off, for non seasonal
#So AR(1) models are chosen for non seasonal.
#ARIMA(1,0,0)X(0,1,1)_s=7
sarima(x, 1,0,0,0,1,1,7)
sarima(x, 0,0,1,0,1,1,7)
sarima(x, 1,0,1,0,1,1,7)
sarima(x, 1,0,2,0,1,1,7)
sarima(x, 2,0,1,0,1,1,7)
sarima(x, 2,0,2,0,1,1,7)
sarima(final_data$traffic_volume, 1,0,2,0,1,1,7, xreg = final_data[, c(1,2,3,4,9,10,12,13,15)], gg = T)
#Plotting the data
tsplot(final_data$traffic_volume, ylab = "Traffic Volume", xlab = "Date", gg = T)
#Stationarity Check
adf.test(final_data$traffic_volume)
#ACF and PACF
acf2(final_data$traffic_volume,  main = "Traffic Volume", gg = T)
#Seasonal Persistence is present, so we try differencing
y <- diff(final_data$traffic_volume, 7)
tsplot(y, ylab = "Seasonal Differenced Traffic Volume", xlab = "Date", gg = T)
acf2(y, main = "Seasonal Differenced Traffic Volume", gg = T)
sarima(final_data$traffic_volume, 1,0,0,0,1,1,7, gg = T)
sarima(final_data$traffic_volume, 0,0,1,0,1,1,7, gg = T)
sarima(final_data$traffic_volume, 1,0,1,0,1,1,7, gg = T)
sarima(final_data$traffic_volume, 2,0,1,0,1,1,7, gg = T)
sarima(final_data$traffic_volume, 1,0,2,0,1,1,7, gg = T)
sarima(final_data$traffic_volume, 2,0,2,0,1,1,7, gg = T)
sarima(final_data$traffic_volume, 2,0,1,0,1,1,7, gg = T)
sarima.for(final_data$traffic_volume, 5,2,0,1,0,1,1,7, gg = T)
sp <- spec.pgram(final_data$traffic_volume,taper=0,log="no")
sort_specs <- sort(sp$spec, decreasing = TRUE)[c(1,2,3)]
p1 <- sp$freq[sp$spec==sort_specs[1]]
p1
p2 <- sp$freq[sp$spec==sort_specs[2]]
p2
p3 <- sp$freq[sp$spec==sort_specs[3]]
p3
1/p1
1/p2
1/p3
make_CI <- function(peak_spec){
u <- qchisq(0.025,2)
l <- qchisq(0.975,2)
c((2*peak_spec)/l,(2*peak_spec)/u)
}
make_CI(sort_specs[1])
make_CI(sort_specs[2])
make_CI(sort_specs[3])
#del<-0.1 # sampling interval
x.spec <- spectrum(final_data$traffic_volume,log="no",span=5, plot=F)
spx <- x.spec$freq
spy <- 2*x.spec$spec
plot(spy~spx,xlab="frequency",ylab="spectral density",type="l", main = "Smoothed Periodogram")
abline(v=x.spec$freq[3], lty=2)
abline(v=x.spec$freq[174], lty=2)
mvspec(final_data$traffic_volume, log="no", main = "Raw Periodogram")
tyt.per <- mvspec(final_data$traffic_volume, kernel('daniell', 2), log="no", main = "Smoothed Periodogram")
abline(v=tyt.per$freq[3], lty=2)
abline(v=tyt.per$freq[174], lty=2)
1/tyt.per$freq[3]
1/tyt.per$freq[174]
tyt.per$bandwidth
df = tyt.per$df
U = qchisq(.025, df)
L = qchisq(.975, df)
tyt.per$spec[3]
tyt.per$spec[174]
df*tyt.per$spec[174]/L
df*tyt.per$spec[174]/U
df*tyt.per$spec[3]/L
df*tyt.per$spec[3]/U
fit <- nnetar(final_data$traffic_volume, lambda=0)
autoplot(forecast(fit,h=30))
repo_path ='/Users/harshbaberwal/Documents/Github/'
repo_path ='/Users/harshbaberwal/Documents/Github/'
setwd(repo_path)
# Importing the data
df <- read.csv('Metro_Interstate_Traffic_Volume.csv')
# Importing the data
df <- read.csv('Data Folder/Metro_Interstate_Traffic_Volume.csv')
repo_path ='/Users/harshbaberwal/Documents/Github/'
repo_path ='/Users/harshbaberwal/Documents/Github/Metro-Interstate-Traffic-Analysis/'
setwd(repo_path)
# Importing the data
df <- read.csv('Data Folder/Metro_Interstate_Traffic_Volume.csv')
# Parsing the date-time as per available format
df$date_time_c = strptime(df$date_time, format = "%Y-%m-%d %H:%M:%S")
# Separating indivudual elements of date-time (or date-hour)
df = df %>% mutate(dt_year = as.integer(format(date_time_c, "%Y")),
dt_month = as.integer(format(date_time_c, "%m")),
dt_day = as.integer(format(date_time_c, "%d")),
dt_hour = as.integer(format(date_time_c, "%H")))
# Creating final date variable
df <- df %>% mutate(date = make_date(dt_year, dt_month, dt_day))
# Mapping the holiday flag to all the hours of a date
ff <- df %>%  mutate(h = if_else(holiday == 'None', 0, 1))%>%
filter(h == 1)%>% select(c("date", "h"))
df <- df  %>% left_join(ff, by = "date") %>%
mutate(holiday = if_else(is.na(h), 0, 1)) %>% select(-c("h", "date"))
# Filtering for hour = 20 i.e. 8.00 PM and for year > 2014
df <- df %>%  filter(dt_hour == 20) %>% select(-c("dt_hour")) %>%
filter(dt_year > 2014) %>% filter(traffic_volume > 1000)
# One-hot Encoding the weather description variable
df = df %>% select(-c("weather_description", "date_time", "date_time_c"))
df$weather_main <- as.factor(df$weather_main)
df = one_hot(as.data.table(df))
# Setting up the final data
final_data <- df %>%
mutate(date = make_date(dt_year, dt_month, dt_day)) %>%
select(-c("dt_year", "dt_month", "dt_day"))
final_data = final_data %>%
group_by(date) %>%
summarize(
holiday=mean(holiday),
temp=mean(temp),
snow_1h=mean(snow_1h),
rain_1h=mean(rain_1h),
clouds_all=mean(clouds_all),
traffic_volume=mean(traffic_volume),
weather_main_Clear=sum(weather_main_Clear),
weather_main_Clouds=sum(weather_main_Clouds),
weather_main_Drizzle=sum(weather_main_Drizzle),
weather_main_Fog=sum(weather_main_Fog),
weather_main_Haze=sum(weather_main_Haze),
weather_main_Mist=sum(weather_main_Mist),
weather_main_Rain=sum(weather_main_Rain),
weather_main_Smoke=sum(weather_main_Smoke),
weather_main_Snow=sum(weather_main_Snow),
weather_main_Thunderstorm=sum(weather_main_Thunderstorm)
)
final_data <- final_data %>% select(-c("snow_1h"))
final_data$t <- year(final_data$date) + yday(final_data$date)/365
t <- final_data$date
# Imputing missing dates and missing values
date_col <- seq.Date(as.Date(t[1]), as.Date(t[length(t)]), by = "day")
df <- data.frame(matrix(NA, ncol = 17))
colnames(df) <- colnames(final_data)
for (d in date_col){
temp <- final_data %>% filter(date == d)
if(nrow(temp) > 0){
df <- rbind(df, temp)
}else{
temp <- c(d, rep(NA, 16))
df <- rbind(df, temp)
}
}
df <- df[2:nrow(df), ]
df[, "date"] <- date_col
final_data <- df
final_data$t <- year(final_data$date) + yday(final_data$date)/365
t <- final_data$date
final_data$date <- final_data$t - mean(final_data$t)
final_data <- na_kalman(final_data)
repo_path ='/Users/harshbaberwal/Documents/Github/Metro-Interstate-Traffic-Analysis/'
setwd(repo_path)
# Importing the data
df <- read.csv('Data Folder/Metro_Interstate_Traffic_Volume.csv')
# Parsing the date-time as per available format
df$date_time_c = strptime(df$date_time, format = "%Y-%m-%d %H:%M:%S")
# Separating indivudual elements of date-time (or date-hour)
df = df %>% mutate(dt_year = as.integer(format(date_time_c, "%Y")),
dt_month = as.integer(format(date_time_c, "%m")),
dt_day = as.integer(format(date_time_c, "%d")),
dt_hour = as.integer(format(date_time_c, "%H")))
# Creating final date variable
df <- df %>% mutate(date = make_date(dt_year, dt_month, dt_day))
# Mapping the holiday flag to all the hours of a date
ff <- df %>%  mutate(h = if_else(holiday == 'None', 0, 1))%>%
filter(h == 1)%>% select(c("date", "h"))
df <- df  %>% left_join(ff, by = "date") %>%
mutate(holiday = if_else(is.na(h), 0, 1)) %>% select(-c("h", "date"))
# Filtering for hour = 20 i.e. 8.00 PM and for year > 2014
df <- df %>%  filter(dt_hour == 20) %>% select(-c("dt_hour")) %>%
filter(dt_year > 2014) %>% filter(traffic_volume > 1000)
# One-hot Encoding the weather description variable
df = df %>% select(-c("weather_description", "date_time", "date_time_c"))
df$weather_main <- as.factor(df$weather_main)
df = one_hot(as.data.table(df))
# Setting up the final data
final_data <- df %>%
mutate(date = make_date(dt_year, dt_month, dt_day)) %>%
select(-c("dt_year", "dt_month", "dt_day"))
final_data = final_data %>%
group_by(date) %>%
summarize(
holiday=mean(holiday),
temp=mean(temp),
snow_1h=mean(snow_1h),
rain_1h=mean(rain_1h),
clouds_all=mean(clouds_all),
traffic_volume=mean(traffic_volume),
weather_main_Clear=sum(weather_main_Clear),
weather_main_Clouds=sum(weather_main_Clouds),
weather_main_Drizzle=sum(weather_main_Drizzle),
weather_main_Fog=sum(weather_main_Fog),
weather_main_Haze=sum(weather_main_Haze),
weather_main_Mist=sum(weather_main_Mist),
weather_main_Rain=sum(weather_main_Rain),
weather_main_Smoke=sum(weather_main_Smoke),
weather_main_Snow=sum(weather_main_Snow),
weather_main_Thunderstorm=sum(weather_main_Thunderstorm)
)
final_data <- final_data %>% select(-c("snow_1h"))
final_data$t <- year(final_data$date) + yday(final_data$date)/365
t <- final_data$date
# final_data$date <- final_data$t - mean(final_data$t)
# Imputing missing dates
date_col <- seq.Date(as.Date(t[1]), as.Date(t[length(t)]), by = "day")
df <- data.frame(matrix(NA, ncol = 17))
colnames(df) <- colnames(final_data)
for (d in date_col){
temp <- final_data %>% filter(date == d)
if(nrow(temp) > 0){
df <- rbind(df, temp)
}else{
temp <- c(d, rep(NA, 16))
df <- rbind(df, temp)
}
}
df <- df[2:nrow(df), ]
df[, "date"] <- date_col
final_data <- df
# Encoding the daye of year using year value and day fraction
final_data$t <- year(final_data$date) + yday(final_data$date)/365
t <- final_data$date
# Mean centering the date variable created
final_data$date <- final_data$t - mean(final_data$t)
# Imputing NAs
final_data <- na_kalman(final_data)
# Writing the final data
write.csv(final_data, 'Data Folder/final_data.csv')
final_data <- read.csv(final_data, 'Data Folder/final_data.csv')
final_data <- read.csv('Data Folder/final_data.csv')
final_data
heatmap(cor(final_data[, 2:16], use = "na.or.complete"))
tsplot(final_data$t, final_data$traffic_volume, ylab = "Traffic Volume", xlab = "Date", gg = T)
summary(final_data$traffic_volume)
x <- resid(modelA)
final_data <- read.csv('Data Folder/final_data.csv')
#Checking for any high correlations
heatmap(cor(final_data[, 2:16], use = "na.or.complete"))
#Plotting the traffic volume against time
tsplot(final_data$t, final_data$traffic_volume, ylab = "Traffic Volume", xlab = "Date", gg = T)
#Checking the stationarity of the series
adf.test(final_data$traffic_volume)
#Basic Numerical Summary
summary(final_data$traffic_volume)
modelA <- lm(traffic_volume ~ holiday + temp + rain_1h + weather_main_Drizzle +
weather_main_Fog + weather_main_Mist + weather_main_Rain +
weather_main_Snow + date, final_data)
# Model Summary
summary(modelA)
# AIC and BIC of the model
print(paste("AIC: ",AIC(modelA)))
print(paste("BIC: ",BIC(modelA)))
# Residual Analysis
x <- resid(modelA)
#Checking the residuals stationarity
adf.test(x)
#Plotting residuals
tsplot(x, ylab = "Residual",  gg = T)
acf2(x, main = "Residual", gg = T)
tsplot(diff(x, 7), ylab = "Seasonal Differenced Residual", gg = T)
acf2(diff(x, 7), main = "Seasonal Differenced Residual", gg = T)
?sarima
#ACF cuts off, PACF tails off, for seasonal
#So MA(1) models are chosen for seasonal.
#PACF cuts off, ACF tails off, for non seasonal
#So AR(1) models are chosen for non seasonal.
#ARIMA(1,0,0)X(0,1,1)_s=7
sarima(x, p=1,d=0,q=0,P=0,D=1,Q=1,S=7)
sarima(x, 0,0,1,0,1,1,7)
sarima(x, 1,0,1,0,1,1,7)
sarima(x, 1,0,2,0,1,1,7)
sarima(x, 2,0,1,0,1,1,7)
sarima(x, 2,0,2,0,1,1,7)
sarima(x, 1,0,2,0,1,1,7)
# Best model for residuals:
sarima(x, 1,0,2,0,1,1,7)
sarima(final_data$traffic_volume, 1,0,2,0,1,1,7, xreg = final_data[, c(1,2,3,4,9,10,12,13,15)], gg = T)
read.csv('Data Folder/final_data.csv')
repo_path ='/Users/harshbaberwal/Documents/Github/Metro-Interstate-Traffic-Analysis/'
setwd(repo_path)
# Importing the data
df <- read.csv('Data Folder/Metro_Interstate_Traffic_Volume.csv')
# Parsing the date-time as per available format
df$date_time_c = strptime(df$date_time, format = "%Y-%m-%d %H:%M:%S")
# Separating indivudual elements of date-time (or date-hour)
df = df %>% mutate(dt_year = as.integer(format(date_time_c, "%Y")),
dt_month = as.integer(format(date_time_c, "%m")),
dt_day = as.integer(format(date_time_c, "%d")),
dt_hour = as.integer(format(date_time_c, "%H")))
# Creating final date variable
df <- df %>% mutate(date = make_date(dt_year, dt_month, dt_day))
# Mapping the holiday flag to all the hours of a date
ff <- df %>%  mutate(h = if_else(holiday == 'None', 0, 1))%>%
filter(h == 1)%>% select(c("date", "h"))
df <- df  %>% left_join(ff, by = "date") %>%
mutate(holiday = if_else(is.na(h), 0, 1)) %>% select(-c("h", "date"))
# Filtering for hour = 20 i.e. 8.00 PM and for year > 2014
df <- df %>%  filter(dt_hour == 20) %>% select(-c("dt_hour")) %>%
filter(dt_year > 2014) %>% filter(traffic_volume > 1000)
# One-hot Encoding the weather description variable
df = df %>% select(-c("weather_description", "date_time", "date_time_c"))
df$weather_main <- as.factor(df$weather_main)
df = one_hot(as.data.table(df))
# Setting up the final data
final_data <- df %>%
mutate(date = make_date(dt_year, dt_month, dt_day)) %>%
select(-c("dt_year", "dt_month", "dt_day"))
final_data = final_data %>%
group_by(date) %>%
summarize(
holiday=mean(holiday),
temp=mean(temp),
snow_1h=mean(snow_1h),
rain_1h=mean(rain_1h),
clouds_all=mean(clouds_all),
traffic_volume=mean(traffic_volume),
weather_main_Clear=sum(weather_main_Clear),
weather_main_Clouds=sum(weather_main_Clouds),
weather_main_Drizzle=sum(weather_main_Drizzle),
weather_main_Fog=sum(weather_main_Fog),
weather_main_Haze=sum(weather_main_Haze),
weather_main_Mist=sum(weather_main_Mist),
weather_main_Rain=sum(weather_main_Rain),
weather_main_Smoke=sum(weather_main_Smoke),
weather_main_Snow=sum(weather_main_Snow),
weather_main_Thunderstorm=sum(weather_main_Thunderstorm)
)
final_data <- final_data %>% select(-c("snow_1h"))
final_data$t <- year(final_data$date) + yday(final_data$date)/365
t <- final_data$date
# final_data$date <- final_data$t - mean(final_data$t)
# Imputing missing dates
date_col <- seq.Date(as.Date(t[1]), as.Date(t[length(t)]), by = "day")
df <- data.frame(matrix(NA, ncol = 17))
colnames(df) <- colnames(final_data)
for (d in date_col){
temp <- final_data %>% filter(date == d)
if(nrow(temp) > 0){
df <- rbind(df, temp)
}else{
temp <- c(d, rep(NA, 16))
df <- rbind(df, temp)
}
}
df <- df[2:nrow(df), ]
df[, "date"] <- date_col
final_data <- df
# Encoding the daye of year using year value and day fraction
final_data$t <- year(final_data$date) + yday(final_data$date)/365
t <- final_data$date
# Mean centering the date variable created
final_data$date <- final_data$t - mean(final_data$t)
# Imputing NAs
final_data <- na_kalman(final_data)
# Writing the final data
write.csv(final_data, 'Data Folder/final_data.csv')
final_data
?write.csv
# Writing the final data
write.csv(final_data, 'Data Folder/final_data.csv', row.names = FALSE)
read.csv('Data Folder/final_data.csv')
final_data <- read.csv('Data Folder/final_data.csv')
#Checking for any high correlations
heatmap(cor(final_data[, 2:16], use = "na.or.complete"))
#Plotting the traffic volume against time
tsplot(final_data$t, final_data$traffic_volume, ylab = "Traffic Volume", xlab = "Date", gg = T)
#Checking the stationarity of the series
adf.test(final_data$traffic_volume)
#Basic Numerical Summary
summary(final_data$traffic_volume)
# Chosen Model
modelA <- lm(traffic_volume ~ holiday + temp + rain_1h + weather_main_Drizzle +
weather_main_Fog + weather_main_Mist + weather_main_Rain +
weather_main_Snow + date, final_data)
# Model Summary
summary(modelA)
# AIC and BIC of the model
print(paste("AIC: ",AIC(modelA)))
print(paste("BIC: ",BIC(modelA)))
# Residual Analysis
x <- resid(modelA)
#Checking the residuals stationarity
adf.test(x)
#Plotting residuals
tsplot(x, ylab = "Residual",  gg = T)
acf2(x, main = "Residual", gg = T)
# Since ACF shows seasonal persistence so, we try differencing
tsplot(diff(x, 7), ylab = "Seasonal Differenced Residual", gg = T)
acf2(diff(x, 7), main = "Seasonal Differenced Residual", gg = T)
#ACF cuts off, PACF tails off, for seasonal
#So MA(1) models are chosen for seasonal.
#PACF cuts off, ACF tails off, for non seasonal
#So AR(1) models are chosen for non seasonal.
#ARIMA(1,0,0)X(0,1,1)_s=7
sarima(x, p=1,d=0,q=0,P=0,D=1,Q=1,S=7)
sarima(x, 0,0,1,0,1,1,7)
sarima(x, 1,0,1,0,1,1,7)
sarima(x, 1,0,2,0,1,1,7)
sarima(x, 2,0,1,0,1,1,7)
sarima(x, 2,0,2,0,1,1,7)
# Best model for residuals:
sarima(x, 1,0,2,0,1,1,7)
# Final Model
sarima(final_data$traffic_volume, 1,0,2,0,1,1,7, xreg = final_data[, c(1,2,3,4,9,10,12,13,15)], gg = T)
final_data[, c(1,2,3,4,9,10,12,13,15)]
head(final_data[, c(1,2,3,4,9,10,12,13,15)])
?sarima.for
sarima.for(final_data$traffic_volume,7, 1,0,2,0,1,1,7, xreg = final_data[, c(1,2,3,4,9,10,12,13,15)], gg = T)
sarima(final_data$traffic_volume, 1,0,2,0,1,1,7, xreg = final_data[, c(1,2,3,4,9,10,12,13,15)], gg = T)
#Plotting the data
tsplot(final_data$traffic_volume, ylab = "Traffic Volume", xlab = "Date", gg = T)
repo_path ='/Users/harshbaberwal/Documents/Github/Metro-Interstate-Traffic-Analysis/'
setwd(repo_path)
final_data <- read.csv('Data Folder/final_data.csv')
tsplot(final_data$traffic_volume, ylab = "Traffic Volume", xlab = "Date", gg = T)
?tsplot
#Plotting the data
tsplot(final_data$traffic_volume, ylab = "Traffic Volume", xlab = "Date", gg = T)
#Stationarity Check
adf.test(final_data$traffic_volume)
acf2(final_data$traffic_volume,  main = "Traffic Volume", gg = T)
y <- diff(final_data$traffic_volume, 7)
tsplot(y, ylab = "Seasonal Differenced Traffic Volume", xlab = "Date", gg = T)
acf2(y, main = "Seasonal Differenced Traffic Volume", gg = T)
sarima(final_data$traffic_volume, 1,0,0,0,1,1,7, gg = T)
# Final Model
sarima(final_data$traffic_volume, 2,0,1,0,1,1,7)
# Forecasting a week ahead
sarima.for(final_data$traffic_volume, 7,2,0,1,0,1,1,7)
repo_path ='/Users/harshbaberwal/Documents/Github/Metro-Interstate-Traffic-Analysis/'
setwd(repo_path)
final_data <- read.csv('Data Folder/final_data.csv')
sp <- spec.pgram(final_data$traffic_volume,taper=0,log="no")
sort_specs <- sort(sp$spec, decreasing = TRUE)[c(1,2,3)]
# Using frequencies with highest spectral density (the 3 peaks) and calculating the period using these.
p1 <- sp$freq[sp$spec==sort_specs[1]]
p1
p2 <- sp$freq[sp$spec==sort_specs[2]]
repo_path ='/Users/harshbaberwal/Documents/Github/Metro-Interstate-Traffic-Analysis/'
setwd(repo_path)
final_data <- read.csv('Data Folder/final_data.csv')
# Calculating the spectral densities
sp <- spec.pgram(final_data$traffic_volume,taper=0,log="no")
# Sorting the densities
sort_specs <- sort(sp$spec, decreasing = TRUE)[c(1,2,3)]
# Using frequencies with highest spectral density (the 3 peaks) and calculating the period using these.
p1 <- sp$freq[sp$spec==sort_specs[1]]
p1
1/p1
p2 <- sp$freq[sp$spec==sort_specs[2]]
p2
1/p2
p3 <- sp$freq[sp$spec==sort_specs[3]]
p3
1/p3
p2 <- sp$freq[sp$spec==sort_specs[2]]
p2
# Since p2 and p3 are very close, we try smoothing and then build the periodogram again
x.spec <- spectrum(final_data$traffic_volume,log="no",span=5, plot=F)
spectrum(final_data$traffic_volume,log="no",span=5, plot=F)
x.spec <- spectrum(final_data$traffic_volume,log="no",span=5, plot=F)
spx <- x.spec$freq
spy <- 2*x.spec$spec
plot(spy~spx,xlab="frequency",ylab="spectral density",type="l", main = "Smoothed Periodogram")
abline(v=x.spec$freq[3], lty=2)
abline(v=x.spec$freq[174], lty=2)
tyt.per$bandwidth
mvspec(final_data$traffic_volume, log="no", main = "Raw Periodogram")
tyt.per <- mvspec(final_data$traffic_volume, kernel('daniell', 2), log="no", main = "Smoothed Periodogram")
abline(v=tyt.per$freq[3], lty=2)
abline(v=tyt.per$freq[174], lty=2)
tyt.per
abline(v=tyt.per$freq[3], lty=2)
abline(v=tyt.per$freq[174], lty=2)
1/tyt.per$freq[3]
1/tyt.per$freq[174]
tyt.per <- mvspec(final_data$traffic_volume, kernel('daniell', 3), log="no", main = "Smoothed Periodogram")
2
#
abline(v=tyt.per$freq[3], lty=2)
1/tyt.per$freq[174]
tyt.per$bandwidth
1/tyt.per$freq[3]
1/tyt.per$freq[174]
## Quick Forecast using Neural Network
fit <- nnetar(final_data$traffic_volume, lambda=0)
autoplot(forecast(fit,h=30))
nn.for <- autoplot(forecast(fit,h=30))
autoplot(nn.for)
nn.for <- forecast(fit,h=30)
autoplot(nn.for)
# Since p2 and p3 are very close, we try smoothing and then build the periodogram again
tyt.per <- mvspec(final_data$traffic_volume, kernel('daniell', 2), log="no", main = "Smoothed Periodogram")
# The frequencies below are chosen for sudden spectrum density peaks by eyeballing tyt.per
abline(v=tyt.per$freq[3], lty=2)
abline(v=tyt.per$freq[174], lty=2)
# Periods
1/tyt.per$freq[3]
1/tyt.per$freq[174]
## Quick Forecast using Neural Network
fit <- nnetar(final_data$traffic_volume, lambda=0)
nn.for <- forecast(fit,h=30)
autoplot(nn.for)
